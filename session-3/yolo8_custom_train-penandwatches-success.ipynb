{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chugmn/iti107-2024s2/blob/main/session-3/yolo8_custom_train-penandwatches-success.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9fM_zdIhpGt"
      },
      "source": [
        "# Object Detection using YOLO\n",
        "\n",
        "Welcome to this week's hands-on lab. In this lab, we are going to learn how to train a balloon detector!\n",
        "\n",
        "At the end of this exercise, you will be able to:\n",
        "\n",
        "- create an object detection dataset in YOLO format\n",
        "- fine-tune a YOLOv8 pretrained model with the custom dataset\n",
        "- monitor the training progress and evaluation metrics\n",
        "- deploy the trained model for object detection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an object detection dataset\n",
        "\n",
        "We will use a sample balloon dataset to illustrate the process of annotation and packaging the dataset into different format for object detection (e.g. YOLO, Pascal VOC, COCO, etc).\n",
        "\n",
        "To annotate, there are many different tools available, such as the very basic [LabelImg](https://github.com/HumanSignal/labelImg) , or the more feature-packed tool such as [Label Studio](https://labelstud.io/), or online service such as [Roboflow](https://roboflow.com/).\n",
        "\n",
        "### Raw Image Dataset\n",
        "\n",
        "You can download the balloon images (without annotations) from this link:\n",
        "\n",
        "https://github.com/nyp-sit/iti107-2024S2/raw/refs/heads/main/data/balloon_raw_dataset.zip\n",
        "\n",
        "Unzip the file to a local folder.\n",
        "\n",
        "There are total of 74 images. You should divide the images into both training and validation set (e.g. 80%-20%, i.e. 59 images for train, and 15 for test).\n"
      ],
      "metadata": {
        "id": "vWSUqz7gPOF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 1: Label Studio\n",
        "\n",
        "You can follow the [steps](https://labelstud.io/guide/quick_start) here to setup Label Studio on your PC. It is recommended to setup a conda environment before you install the Label Studio.  \n",
        "\n",
        "Here are the steps that need to be done:\n",
        "1. Create a new Project\n",
        "2. Import the images into Label Studio\n",
        "3. Set up the Labelling UI tempalte (choose Object Detection with Bounding Box template)\n",
        "4. Export the dataset in YOLO format.\n",
        "\n",
        "The exported dataset will have the following folder structure:\n",
        "```\n",
        "<root folder>\n",
        "classes.txt    --> contains the labels, with each class label on a new line\n",
        "--images --> contains the images\n",
        "--labels --> contains the annotations (i.e. bbox coordinates)\n",
        "notes.json --> some info about this dataset (i.e. not used)\n",
        "```\n",
        "\n",
        "For training with YOLOv8 (from Ultralytics), you need to organize the files into `train` and `validate` (and optionally `test`) folders, and to create a `data.yaml` file to provide information about the folder location of test and validation set:\n",
        "\n",
        "```\n",
        "<root folder>\n",
        "--train\n",
        "----images\n",
        "----labels\n",
        "--valid\n",
        "----images\n",
        "----labels\n",
        "data.yaml\n",
        "```\n",
        "\n",
        "The data.yaml file should specify the following:\n",
        "```\n",
        "train: ../train/images\n",
        "val: ../valid/images\n",
        "test: ../test/images\n",
        "\n",
        "names:\n",
        "    0: balloon\n",
        "```\n",
        "\n",
        "If you have more than one class of object to detect, specify the rest of the names under the names field.\n"
      ],
      "metadata": {
        "id": "GgMJJTcNQ7us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2: Roboflow\n",
        "\n",
        "Alternatively, you can use the online service Roboflow to do annotation. Roboflow integrates very well with Ultralytics and you can easily export the dataset in a format recognized by Ultralytics trainer (for YOLO model)\n",
        "\n",
        "You can create a new account with [Roboflow](https://roboflow.com/).\n",
        "\n",
        "Similarly, you can create a new project, upload all the raw images, annotate them and then export.\n",
        "\n",
        "You can choose the format to be YOLOv8 and choose local directory to download the dataset locally instead of pushing it to the Roboflow universal wish.\n",
        "\n",
        "Here is a [introductory blog](https://blog.roboflow.com/getting-started-with-roboflow/) on using the Roboflow to annotate.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JxOlLlBJb0kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auto Labelling using Grounding DINO\n",
        "\n",
        "Both Label Studio and Roboflow supports the use of Grounding DINO to auto label the dataset.\n",
        "\n",
        "[Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) is open-set object detector, marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs (prompts) such as category names or referring expressions.\n",
        "\n",
        "### Using Grounding DINO with Label Studio\n",
        "\n",
        "You can follow the instruction [here](https://labelstud.io/blog/using-text-prompts-for-image-annotation-with-grounding-dino-and-label-studio/)  to setup the Grounding DINO ML backend to integrate with your label studio.\n",
        "\n",
        "### Using Grounding DINO with Roboflow\n",
        "\n",
        "Here is a [video tutorial](https://youtu.be/SDV6Gz0suAk) on using Grounding DINO with Roboflow.\n"
      ],
      "metadata": {
        "id": "xPGO-z8rlqH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Annotated Dataset\n",
        "\n",
        "To save you time for this lab, you can download a pre-annotated balloon dataset [here](https://github.com/nyp-sit/iti107-2024S2/raw/refs/heads/main/data/balloon_annotated_dataset.zip).\n",
        "\n",
        "We download and unzip to the directory called `datasets`\n",
        "\n"
      ],
      "metadata": {
        "id": "v_VTuEfec2YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "wget https://github.com/chugmn/NYP107Assign/raw/refs/heads/main/penorwatches.v2i.yolov8.zip\n",
        "mkdir -p datasets\n",
        "unzip penorwatches.v2i.yolov8.zip -d datasets/"
      ],
      "metadata": {
        "id": "yVh2OFXvtiM4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0BTQ6QpZz3kP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install ultralytics\n",
        "!pip install comet_ml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model\n",
        "\n",
        "YOLOv8 comes with different sizes of pretrained models: yolov8n, yolov8s, .... They differs in terms of their sizes, inference speeds and mean average precision:\n",
        "\n",
        "<img src=\"https://github.com/nyp-sit/iti107-2024S2/blob/main/assets/yolo-models.png?raw=true\" width=\"70%\"/>\n",
        "\n",
        "\n",
        "We will use the small pretrained model yolo8s and finetune it on our custom dataset.\n"
      ],
      "metadata": {
        "id": "YgyWGNT4-MLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup the logging\n",
        "\n",
        "Ultralytics support logging to `wandb`, `comet.ml` and `tensorboard`, out of the box. Here we only enable wandb.\n",
        "\n",
        "You need to create an account at [`wandb`](https://wandb.ai) and get the API key from https://wandb.ai/authorize.\n"
      ],
      "metadata": {
        "id": "pKnPWyDwUfvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import settings\n",
        "\n",
        "settings.update({\"wandb\": True,\n",
        "                 \"comet.ml\": False,\n",
        "                 \"tensorboard\": False})"
      ],
      "metadata": {
        "id": "FbMwi27fRsyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3ed25a-1426-430d-8869-41f2e0172fb8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "We specify the path to `data.yaml` file, and train with a batch size of 15, and we also save the checkpoint at each epoch (save_period=1). We assume here you are connected to a GPU, hence we can specify the device to use as `device=0` to select the first GPU.  We specify the project name as `balloon`, this will create a folder called `balloon` to store the weights and various training artifacts such as F1, PR curves, confusion matrics, training results (loss, mAP, etc).\n",
        "\n",
        "For a complete listing of train settings, you can see [here](https://docs.ultralytics.com/modes/train/#train-settings).\n",
        "\n",
        "You can also specify the type of data [augmentation](https://docs.ultralytics.com/modes/train/#augmentation-settings-and-hyperparameters)  you want as part of the train pipeline.\n",
        "\n",
        "You can monitor your training progress at wandb (the link is given in the train output below)\n"
      ],
      "metadata": {
        "id": "iq9gV4A1VHlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la datasets/valid/images | wc -l"
      ],
      "metadata": {
        "id": "zrnbwsmcpYE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913246e3-8114-4afe-db38-efaa173e4a97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NwjnKk02x-cF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ab0921d-06c5-4991-c11a-22c5a9d0bb81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.49 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=datasets/data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=1, cache=False, device=0, workers=8, project=penandwatches, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=penandwatches/train2\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2116822  ultralytics.nn.modules.head.Detect           [2, [128, 256, 512]]          \n",
            "Model summary: 225 layers, 11,136,374 parameters, 11,136,358 gradients, 28.6 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "WARNING ⚠️ Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohan-chugani\u001b[0m (\u001b[33mmohan-chugani-nanyang-polytechnic\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241213_104327-6liguqz5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohan-chugani-nanyang-polytechnic/penandwatches/runs/6liguqz5' target=\"_blank\">train2</a></strong> to <a href='https://wandb.ai/mohan-chugani-nanyang-polytechnic/penandwatches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mohan-chugani-nanyang-polytechnic/penandwatches' target=\"_blank\">https://wandb.ai/mohan-chugani-nanyang-polytechnic/penandwatches</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mohan-chugani-nanyang-polytechnic/penandwatches/runs/6liguqz5' target=\"_blank\">https://wandb.ai/mohan-chugani-nanyang-polytechnic/penandwatches/runs/6liguqz5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/train/labels.cache... 29 images, 0 backgrounds, 0 corrupt: 100%|██████████| 29/29 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 2, len(boxes) = 45. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/valid/labels.cache... 9 images, 0 backgrounds, 0 corrupt: 100%|██████████| 9/9 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to penandwatches/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mpenandwatches/train2\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/10      4.11G      2.243       6.29      2.546         20        640: 100%|██████████| 2/2 [00:01<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9         15      0.358     0.0714     0.0798     0.0419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/10      3.92G      1.732      5.701      2.078         21        640: 100%|██████████| 2/2 [00:00<00:00,  3.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  4.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9         15      0.346     0.0714     0.0807     0.0412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/10      3.91G      2.108      6.516      2.383         22        640: 100%|██████████| 2/2 [00:00<00:00,  3.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  8.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9         15      0.336     0.0714     0.0832     0.0439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/10       3.9G      1.833      5.615       2.11         19        640: 100%|██████████| 2/2 [00:00<00:00,  3.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 10.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9         15      0.187     0.0714     0.0826     0.0434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/10      3.93G      1.832       5.14      2.086         19        640: 100%|██████████| 2/2 [00:00<00:00,  2.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9         15      0.804      0.143      0.138     0.0777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/10      3.99G      1.239      3.159      1.804         19        640: 100%|██████████| 2/2 [00:00<00:00,  2.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  4.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9         15      0.964     0.0714      0.146      0.104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/10      4.15G      1.049      2.451      1.476         17        640: 100%|██████████| 2/2 [00:00<00:00,  2.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9         15      0.312      0.203      0.183      0.117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/10      3.99G      1.011      2.259      1.647         20        640: 100%|██████████| 2/2 [00:00<00:00,  3.48it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9         15      0.545      0.348      0.222      0.118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/10      4.15G     0.9014      1.893      1.507         21        640: 100%|██████████| 2/2 [00:00<00:00,  3.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  9.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9         15      0.637        0.4      0.262      0.146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/10      4.05G      1.118       2.08      1.802         19        640: 100%|██████████| 2/2 [00:00<00:00,  3.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  8.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9         15      0.647      0.264      0.344      0.214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 epochs completed in 0.010 hours.\n",
            "Optimizer stripped from penandwatches/train2/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from penandwatches/train2/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating penandwatches/train2/weights/best.pt...\n",
            "Ultralytics 8.3.49 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11,126,358 parameters, 0 gradients, 28.4 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9         15      0.641      0.319      0.324      0.208\n",
            "                   pen          7          7      0.282      0.395      0.335      0.199\n",
            "                 watch          8          8          1      0.243      0.314      0.217\n",
            "Speed: 0.2ms preprocess, 4.7ms inference, 0.0ms loss, 2.3ms postprocess per image\n",
            "Results saved to \u001b[1mpenandwatches/train2\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>▁▄▆▇███▇▅▃</td></tr><tr><td>lr/pg1</td><td>▁▄▆▇███▇▅▃</td></tr><tr><td>lr/pg2</td><td>▁▄▆▇███▇▅▃</td></tr><tr><td>metrics/mAP50(B)</td><td>▁▁▁▁▃▃▄▅▆█</td></tr><tr><td>metrics/mAP50-95(B)</td><td>▁▁▁▁▃▄▄▄▅█</td></tr><tr><td>metrics/precision(B)</td><td>▃▂▂▁▇█▂▄▅▅</td></tr><tr><td>metrics/recall(B)</td><td>▁▁▁▁▃▁▄▇█▆</td></tr><tr><td>model/GFLOPs</td><td>▁</td></tr><tr><td>model/parameters</td><td>▁</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>▁</td></tr><tr><td>train/box_loss</td><td>█▅▇▆▆▃▂▂▁▂</td></tr><tr><td>train/cls_loss</td><td>█▇█▇▆▃▂▂▁▁</td></tr><tr><td>train/dfl_loss</td><td>█▅▇▅▅▃▁▂▁▃</td></tr><tr><td>val/box_loss</td><td>▆▅▄█▁▄▅▇█▇</td></tr><tr><td>val/cls_loss</td><td>▄▅▅▅█▆▄▂▂▁</td></tr><tr><td>val/dfl_loss</td><td>▁▁▁▃▂▄▅██▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>3e-05</td></tr><tr><td>lr/pg1</td><td>3e-05</td></tr><tr><td>lr/pg2</td><td>3e-05</td></tr><tr><td>metrics/mAP50(B)</td><td>0.32443</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.2078</td></tr><tr><td>metrics/precision(B)</td><td>0.64085</td></tr><tr><td>metrics/recall(B)</td><td>0.31888</td></tr><tr><td>model/GFLOPs</td><td>28.649</td></tr><tr><td>model/parameters</td><td>11136374</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>18.895</td></tr><tr><td>train/box_loss</td><td>1.11802</td></tr><tr><td>train/cls_loss</td><td>2.08041</td></tr><tr><td>train/dfl_loss</td><td>1.80152</td></tr><tr><td>val/box_loss</td><td>1.55812</td></tr><tr><td>val/cls_loss</td><td>2.92115</td></tr><tr><td>val/dfl_loss</td><td>2.08822</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">train2</strong> at: <a href='https://wandb.ai/mohan-chugani-nanyang-polytechnic/penandwatches/runs/6liguqz5' target=\"_blank\">https://wandb.ai/mohan-chugani-nanyang-polytechnic/penandwatches/runs/6liguqz5</a><br/> View project at: <a href='https://wandb.ai/mohan-chugani-nanyang-polytechnic/penandwatches' target=\"_blank\">https://wandb.ai/mohan-chugani-nanyang-polytechnic/penandwatches</a><br/>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 17 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241213_104327-6liguqz5/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "from ultralytics import settings\n",
        "\n",
        "model = YOLO(\"yolov8s.pt\")  # Load a pre-trained YOLO model\n",
        "result = model.train(data=\"datasets/data.yaml\",\n",
        "                     epochs=10,\n",
        "                     save_period=1,\n",
        "                     batch=16,\n",
        "                     device=0,\n",
        "                     project='penandwatches',\n",
        "                     plots=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see the various graphs in your wandb dashboard, for example:\n",
        "\n",
        "*metrics*\n",
        "\n",
        "<img src=\"https://github.com/nyp-sit/iti107-2024S2/blob/main/assets/wandb-metrics.png?raw=true\"/>\n",
        "\n",
        "*Train and validation loss*\n",
        "\n",
        "<img src=\"https://github.com/nyp-sit/iti107-2024S2/blob/main/assets/wandb-loss.png?raw=true\"/>"
      ],
      "metadata": {
        "id": "59BONHriXz3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can go to the folder `balloon-->train-->weights` and you will files like epoch0.pt, epoch1.pt, .... and also best.pt.\n",
        "The epoch0.pt, epoch1.pt are the checkpoints that are saved every period (in our case, we specify period as 1 epoch).  The best.pt contains the best checkpoint."
      ],
      "metadata": {
        "id": "P3QivGwNaY3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run the best model (using the best checkpoint) against the validation dataset to see the overall model performance on validation set.  \n",
        "\n",
        "You should see around `0.88` for `mAP50`, and `0.78` for `mAP50-95`."
      ],
      "metadata": {
        "id": "G-PxfxzMbAOS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-n6GRS5f2f05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2cb634-0666-4df2-e470-ae5c60610150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.49 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11,126,358 parameters, 0 gradients, 28.4 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/valid/labels.cache... 9 images, 0 backgrounds, 0 corrupt: 100%|██████████| 9/9 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  3.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9         15      0.664      0.297      0.369       0.22\n",
            "                   pen          7          7      0.328      0.353      0.421      0.222\n",
            "                 watch          8          8          1       0.24      0.316      0.218\n",
            "Speed: 2.2ms preprocess, 14.9ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"penandwatches/train/weights/best.pt\")\n",
        "validation_results = model.val(data=\"datasets/data.yaml\", device=\"0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export and Deployment\n",
        "\n",
        "Your model is in pytorch format (.pt). You can export the model to various format, e.g. TorchScript, ONNX, OpenVINO, TensorRT, etc. depending on your use case, and deployment platform (e.g. CPU or GPU, etc)\n",
        "\n",
        "You can see the list of [supported formats](https://docs.ultralytics.com/modes/export/#export-formats)  and the option they support in terms of further optimization (such as imagesize, int8, half-precision, etc) in the ultralytics site.\n",
        "\n",
        "Ultralytics provide a utility function to benchmark your model using different supported formats automatically. You can uncomment the code in the following code cell to see the benchmark result. If you are benchmarking for CPU only, the change the `device=0` to `device='cpu'`.  \n",
        "\n",
        "**Beware: it will take quite a while to complete the benchmark**"
      ],
      "metadata": {
        "id": "bio2cKcnb-Z-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usgkfg87bZBO"
      },
      "outputs": [],
      "source": [
        "# from ultralytics.utils.benchmarks import benchmark\n",
        "\n",
        "# # Benchmark on GPU (device=0 means the 1st GPU device)\n",
        "# benchmark(model=\"balloon/train/weights/best.pt\", data=\"datasets/data.yaml\", imgsz=640, half=False, device=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the following code, we export it as OpenVINO. OpenVINO is optimized for inference on Intel CPUs and since we will use the model later on to do inference on local Windows machine (which runs Intel chip), we will export it as OpenVINO format. We also specify using int8 quantization, which results in faster inference, at the cost of accuracy.\n",
        "\n",
        "For more information on OpenVINO, go to the [official documentation](https://docs.openvino.ai/2024/index.html).\n",
        "\n",
        "After export, you can find the openvino model in `balloon\\train\\weights\\best_openvino_model` directory."
      ],
      "metadata": {
        "id": "OD-vKnPYfBu3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6PDNf5NaPch"
      },
      "outputs": [],
      "source": [
        "model = YOLO(\"penandwatches/train/weights/best.pt\")\n",
        "exported_path = model.export(format=\"openvino\", int8=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Let's test our model on some sample pictures. You can optionally specify the confidence threshold (e.g. `conf=0.5`), and the IoU (e.g. `iou=0.6`) for the NMS. The model will only output the bounding boxes of those detection that exceeds the confidence threshould and the IoU threshold.  "
      ],
      "metadata": {
        "id": "r11lPCYrfMKN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziKNFammhMxv"
      },
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "\n",
        "source = 'https://raw.githubusercontent.com/nyp-sit/iti107-2024S2/refs/heads/main/session-3/samples/sample_balloon.jpeg'\n",
        "#source = './samples/sample_balloon.jpeg'\n",
        "model = YOLO(\"balloon/train/weights/best_int8_openvino_model\", task='detect')\n",
        "result = model(source, conf=0.5, iou=0.6)\n",
        "\n",
        "# Visualize the results\n",
        "for i, r in enumerate(result):\n",
        "    print(r)\n",
        "    # Plot results image\n",
        "    im_bgr = r.plot()  # BGR-order numpy array\n",
        "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
        "\n",
        "    # Show results to screen (in supported environments)\n",
        "    r.show()\n",
        "\n",
        "    # Save results to disk\n",
        "    r.save(filename=f\"results{i}.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Model\n",
        "\n",
        "If you are training your model on Google Colab, you will download the exported OpenVINO model to a local PC. If you are training your model locally, then the exported model should already be on your local PC.\n",
        "\n",
        "Run the following code to zip up the OpenVINO folder and download to local PC."
      ],
      "metadata": {
        "id": "ygXaw7lxu8HT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: If you encountered error message \"NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\", uncomment the following cell and run it.*\n"
      ],
      "metadata": {
        "id": "xTTV-ndvuOx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "_O-nmcYRuMIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mv ./balloon/train/weights/best_int8_openvino_model/ .\n",
        "zip -r best_int8_openvino_model.zip best_int8_openvino_model\n",
        "\n",
        "# Now go to best_openvino_model to download the best_openvino_model.zip file"
      ],
      "metadata": {
        "id": "j_CI0ZBTteA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui8i_pB5sItu"
      },
      "source": [
        "## Streaming\n",
        "\n",
        "We can also do real-time detection on a video or camera steram.\n",
        "\n",
        "The code below uses openCV library to display video in a window, and can only be run locally on a local laptop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video File\n",
        "\n",
        "You need `OpenCV` to run the following code.  In your conda environment, install `opencv` for python using the following command:\n",
        "\n",
        "```\n",
        "pip3 install opencv-python\n",
        "```\n",
        "or\n",
        "```\n",
        "conda install opencv\n",
        "```\n",
        "\n",
        "Let's donwload the sample video file."
      ],
      "metadata": {
        "id": "MmzL3StT5_CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/nyp-sit/iti107-2024S2/refs/heads/main/session-3/samples/balloon.mp4"
      ],
      "metadata": {
        "id": "DNK13LNd87G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming and display video"
      ],
      "metadata": {
        "id": "2nvf5BvM9_q2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TWG6JBVPE86"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "# Load the YOLO model\n",
        "model = YOLO(\"best_int8_openvino_model\", task=\"detect\")\n",
        "\n",
        "# Open the video file\n",
        "video_path = \"balloon.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Loop through the video frames\n",
        "while cap.isOpened():\n",
        "    # Read a frame from the video\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    if success:\n",
        "        # Run YOLO inference on the frame on GPU Device 0\n",
        "        results = model(frame, device=\"cpu\")\n",
        "\n",
        "        # Visualize the results on the frame\n",
        "        annotated_frame = results[0].plot()\n",
        "\n",
        "        # Display the annotated frame\n",
        "        cv2.imshow(\"YOLO Inference\", annotated_frame)\n",
        "\n",
        "        # Break the loop if 'q' is pressed\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "    else:\n",
        "        # Break the loop if the end of the video is reached\n",
        "        break\n",
        "\n",
        "# Release the video capture object and close the display window\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detect and write to a video file"
      ],
      "metadata": {
        "id": "AZoRxssK98O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "# from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def write_video(video_in_filepath, video_out_filepath, model):\n",
        "    # Open the video file\n",
        "\n",
        "    video_reader = cv2.VideoCapture(video_in_filepath)\n",
        "\n",
        "    nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    fps = video_reader.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    video_writer = cv2.VideoWriter(video_out_filepath,\n",
        "                            cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                            fps,\n",
        "                            (frame_w, frame_h))\n",
        "\n",
        "    # Loop through the video frames\n",
        "    for i in tqdm(range(nb_frames)):\n",
        "        # Read a frame from the video\n",
        "        success, frame = video_reader.read()\n",
        "\n",
        "        if success:\n",
        "            # Run YOLO inference on the frame on GPU Device 0\n",
        "            results = model(frame, conf=0.9, device=0)\n",
        "\n",
        "            # Visualize the results on the frame\n",
        "            annotated_frame = results[0].plot()\n",
        "\n",
        "            # Write the annotated frame\n",
        "            video_writer.write(annotated_frame)\n",
        "\n",
        "    video_reader.release()\n",
        "    video_writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    cv2.waitKey(1)\n"
      ],
      "metadata": {
        "id": "fqXYqdRC64mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "video_in_file = \"balloon.mp4\"\n",
        "basename = Path(video_in_file).stem\n",
        "video_out_file = os.path.join(basename + '_detected' + '.mp4')\n",
        "model = YOLO(\"best_int8_openvino_model\", task=\"detect\")\n",
        "write_video(video_in_file, video_out_file, model)"
      ],
      "metadata": {
        "id": "oT_bq-gC8q2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Sr-_DGxyRT7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}