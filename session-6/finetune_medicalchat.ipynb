{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb6573a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107-2024s2/blob/main/session-6/finetune_medicalchat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28304084-e6f6-45cf-9076-8dc439ef13e4",
   "metadata": {
    "id": "28304084-e6f6-45cf-9076-8dc439ef13e4"
   },
   "source": [
    "# Fine-Tune a Causal Language Model for Medical Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73e16b-474b-40e9-bbaf-6f43479f018b",
   "metadata": {
    "id": "dc73e16b-474b-40e9-bbaf-6f43479f018b"
   },
   "source": [
    "In this exercise, you will fine-tune Meta's Llama 3.2 LLM to be a medical chatbot. We will explore how to use the Huggingface TRL (Transformer Reinforcement Learning) library to help us to perform Supervised Finetuning (SFT).  We will explore the use of Parameter Efficient Fine-Tuning (PEFT) for efficient and fast finetuning.\n",
    "\n",
    "Before you start the exercise, make sure you have requested to access the Llama 3.2 model. If you have not done so, go to the [model page](https://huggingface.co/meta-llama/Llama-3.2-1B) and fill up your personal info and agree to the license agreement. You may need to wait for a few minutes before the access is granted. You can check the status using the [gated repo link](https://huggingface.co/settings/gated-repos).\n",
    "\n",
    "You also need to create an access token and use the access token to login to the huggingface hub to access the model in the codes below. You can create the access token at your profile page, under access tokens, or use this [link](https://huggingface.co/settings/tokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022ce84-192b-4f62-8385-2cf571289117",
   "metadata": {
    "id": "f022ce84-192b-4f62-8385-2cf571289117"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q accelerate peft transformers trl wandb\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ed7ea-3707-445a-a266-e4181546f2a7",
   "metadata": {
    "id": "c84ed7ea-3707-445a-a266-e4181546f2a7"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spRsM0cWyf_t",
   "metadata": {
    "id": "spRsM0cWyf_t"
   },
   "outputs": [],
   "source": [
    "# login into huggingface hub using your access token\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12011343-2a10-426a-bf38-6fc3f43766f7",
   "metadata": {
    "id": "12011343-2a10-426a-bf38-6fc3f43766f7"
   },
   "source": [
    "## Templating Instruction Data\n",
    "\n",
    "To fine-tune a base LLM to follow instructions, we will need to prepare instruction data that follows a chat template.\n",
    "\n",
    "<img src=\"https://github.com/nyp-sit/iti107-2024S2/blob/main/assets/chat_template.png?raw=true\" />\n",
    "\n",
    "This chat template differentiates between what the LLM generates and what the user generates. Many LLM chat models that are available on HuggingFace comes with built-in chat template that you can use.\n",
    "\n",
    "You can read more about chat templates [here](https://huggingface.co/docs/transformers/v4.46.3/chat_templating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16862cf7-29c7-47b0-8f33-053428b2bd78",
   "metadata": {
    "id": "16862cf7-29c7-47b0-8f33-053428b2bd78"
   },
   "outputs": [],
   "source": [
    "# This is the chat model of Llama-3.2-1B-Instruct. We only load it because we want to use it's chat template to format our data\n",
    "chat_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "base_model = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff006d-9431-4748-9f4f-b0ce081d7295",
   "metadata": {
    "id": "3fff006d-9431-4748-9f4f-b0ce081d7295"
   },
   "outputs": [],
   "source": [
    "template_tokenizer = AutoTokenizer.from_pretrained(chat_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "chat_template = template_tokenizer.get_chat_template()\n",
    "print(chat_template)\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2045b-9741-427c-8514-46c3d93d9ec0",
   "metadata": {
    "id": "9da2045b-9741-427c-8514-46c3d93d9ec0"
   },
   "source": [
    "The template is written in Jinja (a templating language). You can see that the template consists of some special tokens such as `<|start_header_id|>`, `<|end_header_id|`.  These are used to specify the roles, such as `user`, `assistant`, `system`. There is also a special token `<|eot_id|>`, which basically signify end of sentence.\n",
    "This template also allows the use of tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73a6ad-8524-4487-85e4-68304625476f",
   "metadata": {
    "id": "8e73a6ad-8524-4487-85e4-68304625476f"
   },
   "source": [
    "#### Using ChatML template (optional)\n",
    "\n",
    "ChatML template (from OpenAI) is a very common template used in LLM chatbot model. The tempate that looks like this:\n",
    "```\n",
    "{%- for message in messages %}\n",
    "    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n",
    "{%- endfor %}\n",
    "```\n",
    "\n",
    "We can set your base model tokenizer to use this template instead.  Here is how you can do it in a single line.\n",
    "\n",
    "```\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624bf29-4d62-4744-a67e-39992afc6970",
   "metadata": {
    "id": "1624bf29-4d62-4744-a67e-39992afc6970"
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "# tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "# print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592d791-896d-41d2-8db8-b1d73e49971e",
   "metadata": {
    "id": "c592d791-896d-41d2-8db8-b1d73e49971e"
   },
   "source": [
    "### Format the data according to chat template\n",
    "\n",
    "Let's download our data and format them according to the template given. We select a subset of 1500 samples to reduce training time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca8896-e690-4f1a-9d75-a31c90d2a83d",
   "metadata": {
    "id": "1fca8896-e690-4f1a-9d75-a31c90d2a83d"
   },
   "outputs": [],
   "source": [
    "dataset_name = \"ruslanmv/ai-medical-chatbot\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=128).select(range(3000)).train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62453019-0e7f-4926-9a9d-96e26bf3f28f",
   "metadata": {
    "id": "62453019-0e7f-4926-9a9d-96e26bf3f28f"
   },
   "outputs": [],
   "source": [
    "dataset_train = dataset['train']\n",
    "dataset_val = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6ebda-b3a7-4b57-a43d-7ec54552a04c",
   "metadata": {
    "id": "70a6ebda-b3a7-4b57-a43d-7ec54552a04c"
   },
   "source": [
    "Let's define a map function to map the data fields to the prompt template.\n",
    "Note that the completed prompt is put under 'text' field of the json. This is the default field that model will look for the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6928d0-c83b-41e8-a2c3-a63827b6c851",
   "metadata": {
    "id": "7c6928d0-c83b-41e8-a2c3-a63827b6c851"
   },
   "outputs": [],
   "source": [
    "def format_chat_template(row):\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful medical doctor\"},\n",
    "        {\"role\": \"user\", \"content\": row[\"Patient\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=True)\n",
    "    # print(prompt)\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debee8c3-bec5-420a-9535-e6bbb7afe3f8",
   "metadata": {
    "id": "debee8c3-bec5-420a-9535-e6bbb7afe3f8"
   },
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.map(format_chat_template, remove_columns=list(dataset_train.features))\n",
    "dataset_val = dataset_val.map(format_chat_template, remove_columns=list(dataset_val.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2265ec5-a1d7-429a-b4d5-4757377b2ced",
   "metadata": {
    "id": "a2265ec5-a1d7-429a-b4d5-4757377b2ced"
   },
   "source": [
    "Using the \"text\" column, we can explore these formatted prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2558fe-dd49-436e-82ea-220564ea69d0",
   "metadata": {
    "id": "ed2558fe-dd49-436e-82ea-220564ea69d0"
   },
   "outputs": [],
   "source": [
    "dataset_train[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc659d-5e7c-4114-a4ae-2c27ccf4a2c1",
   "metadata": {
    "id": "33dc659d-5e7c-4114-a4ae-2c27ccf4a2c1"
   },
   "source": [
    "### Model Quantization\n",
    "\n",
    "Now that we have our data, we can start loading in our model. This is where we apply the Q in QLoRA, namely quantization. We use the\n",
    "bitsandbytes package to compress the pretrained model to a 4-bit representation.\n",
    "\n",
    "In BitsAndBytesConfig, you can define the quantization scheme. We follow the steps used in the original QLoRA paper and load the model in 4-bit (load_in_4bit) with a normalized float representation (bnb_4bit_quant_type) and double quantization (bnb_4bit_use_double_quant).\n",
    "\n",
    "For an excellent explanation of quantization, read the blog post \"[A Visual Guide to Quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization)\" by Maarten Grootendorst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96ae31-af8a-40f7-a39e-b4649ba1b837",
   "metadata": {
    "id": "ac96ae31-af8a-40f7-a39e-b4649ba1b837"
   },
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# 4-bit quantization configuration - Q in QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Use 4-bit precision model loading\n",
    "    bnb_4bit_quant_type=\"nf4\", # Quantization type\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Compute dtype\n",
    "    bnb_4bit_use_double_quant=True, # Apply nested quantization\n",
    ")\n",
    "\n",
    "# Load the model to train on the GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    # Leave this out for regular SFT\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e19a36-b88b-4c9b-94f1-fc7c4775bac5",
   "metadata": {
    "id": "37e19a36-b88b-4c9b-94f1-fc7c4775bac5"
   },
   "source": [
    "### Test the Model with Zero Shot Inferencing\n",
    "\n",
    "Let's test the base model (non-instruction tuned model) with zero shot inferencing (i.e. ask it to summarize without giving any example. You can see that the model struggles to respond to user's question, and just repeating what the user has entered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac82a2-132d-422b-a764-444d66fc918a",
   "metadata": {
    "id": "91ac82a2-132d-422b-a764-444d66fc918a"
   },
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"I have stomach pain. What should I do?\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "model.eval()\n",
    "with torch.no_grad():   # no gradient update\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=200)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9cad99-f525-4de6-9458-23affaa28b47",
   "metadata": {
    "id": "1c9cad99-f525-4de6-9458-23affaa28b47"
   },
   "source": [
    "### LoRA Configuration\n",
    "\n",
    "We will be using LoRA to train our model. LoRA is supported in Hugging Face's PEFT library.\n",
    "Here are some explanation about the parameters used in the LoRA:\n",
    "- `r` - This is the rank of the compressed matrices. Increasing this value will also increase the sizes of compressed matrices leading to less compression and thereby improved representative power. Values typically range between 4 and 64.\n",
    "- `lora_alpha` - Controls the amount of change that is added to the original weights. In essence, it balances the knowledge of the original model with that of the new task. A rule of thumb is to choose a value twice the size of r.\n",
    "- `target_modules` - Controls which layers to target. The LoRA procedure can choose to ignore specific layers, like specific projection layers. This can speed up training but reduce performance and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b180947c-ddc3-4bda-980f-5afd27199d21",
   "metadata": {
    "id": "b180947c-ddc3-4bda-980f-5afd27199d21"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# Prepare LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,  # LoRA Scaling\n",
    "    lora_dropout=0.1,  # Dropout for LoRA Layers\n",
    "    r=64,  # Rank\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=  # Layers to target\n",
    "     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lAwFsL21kfbO",
   "metadata": {
    "id": "lAwFsL21kfbO"
   },
   "source": [
    "Let's compare the number of trainable parameters of the PEFT model vs the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Juhd3omkTHU",
   "metadata": {
    "id": "-Juhd3omkTHU"
   },
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f12c6f-df14-4b1d-8e1d-a15c22bd029e",
   "metadata": {
    "id": "d6f12c6f-df14-4b1d-8e1d-a15c22bd029e"
   },
   "source": [
    "### Training Configuration\n",
    "\n",
    "Next we need to set our training configuration. Since we are going to use SFTTrainer, we can specify the training arguments in SFTConfig.\n",
    "\n",
    "Note that we set `fp16` to True for mixed-precision training. If you are using Ampere and newer GPU architecture, you can set `bf16` to better accuracy and faster training.\n",
    "\n",
    "Modern LLM has quite a large context window, typically more than a 100K. Many of the text sample we encountered are very much shorter than that. For more efficient use of the context window, Instead of having one text per sample in the batch and then padding to either the longest text or the maximal context of the model, we concatenate a lot of texts with a EOS token in between and cut chunks of the context size to fill the batch without any padding.\n",
    "\n",
    "<img src=\"https://github.com/nyp-sit/iti107-2024S2/blob/main/assets/packing.png?raw=1\" width=\"700\"/>\n",
    "\n",
    "TRL allows us to do this packing very easily, by just specifying `packing=True`.  Internally, a [`ConstantLengthDataset`](https://huggingface.co/docs/trl/en/sft_trainer#trl.trainer.ConstantLengthDataset) is being created so we can iterate over the dataset on fixed-length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w2e2RJMO_VyJ",
   "metadata": {
    "id": "w2e2RJMO_VyJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "# Reduce VRAM usage by reducing fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Set up WANDB project settings\n",
    "# os.environ[\"WANDB_PROJECT\"] = \"llama3.2-summarize\"\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"Your secret wandb key\"\n",
    "\n",
    "## convenience method to generate unique run name for WanDB\n",
    "def get_run_id():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "    return run_id\n",
    "\n",
    "# You can navigate to https://wandb.ai/authorize to get your key\n",
    "wb_token = '90c8e9188f485d7fef8cd4d76beac203d1dd589e'\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project='Llama3.2_Finetune_doctor_chat',\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f9d4e-3472-4898-9be2-f9f40a27fe32",
   "metadata": {
    "id": "de5f9d4e-3472-4898-9be2-f9f40a27fe32"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Configure the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# where to write the checkpoint to\n",
    "output_dir = \"./results\"\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=5,\n",
    "    report_to=\"wandb\",\n",
    "    max_steps=30,\n",
    "    bf16=True,\n",
    "    # fp16=True\n",
    "    gradient_checkpointing=True,\n",
    "    resume_from_checkpoint=True,\n",
    "    packing=True,\n",
    "    eval_packing=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=10,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290aac1-0c8e-4d90-ba70-1b03757c9d32",
   "metadata": {
    "id": "9290aac1-0c8e-4d90-ba70-1b03757c9d32"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    # dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    # Leave this out for regular SFT\n",
    "    peft_config=peft_config,\n",
    "    args=sft_config\n",
    ")\n",
    "\n",
    "# switch the model in train mode\n",
    "trainer.model.train()\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cac77f-d8b5-4c66-bfcf-5c95c0037ae6",
   "metadata": {
    "id": "36cac77f-d8b5-4c66-bfcf-5c95c0037ae6"
   },
   "outputs": [],
   "source": [
    "# Save QLoRA weights\n",
    "trainer.model.save_pretrained(\"Llama-3.2-1B-chat-doctor-QLoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824fdf9c-dde8-45dd-a154-b9e5e5bd2fe8",
   "metadata": {
    "id": "824fdf9c-dde8-45dd-a154-b9e5e5bd2fe8"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Llama-3.2-1B-chat-doctor-QLoRA\").to('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e92b4cc-a0d7-4757-bf21-a08c2e7e0c1d",
   "metadata": {
    "id": "1e92b4cc-a0d7-4757-bf21-a08c2e7e0c1d"
   },
   "source": [
    "### Save the model in HuggingFace hub\n",
    "\n",
    "Uncomment the following to push your model to the hub.  Change the path to your hugging face ID, e.g. khengkok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004c4c1-d434-4f71-9068-933df0dc7d15",
   "metadata": {
    "id": "0004c4c1-d434-4f71-9068-933df0dc7d15"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Logging using your HF access token\n",
    "notebook_login()\n",
    "\n",
    "# push the model to hub, change <HuggingFaceID> to your own userid\n",
    "model.push_to_hub(\"<HuggingFaceID>/Llama-3.2-1B-chat-doctor-QLoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0f3bc-21fe-43b4-9dcb-f7fac196c4ba",
   "metadata": {
    "id": "c2b0f3bc-21fe-43b4-9dcb-f7fac196c4ba"
   },
   "source": [
    "### Merge Weights\n",
    "\n",
    "After we have trained our QLoRA weights, we still need to combine them with the original weights to use them. We reload the model in 16 bits, instead of the quantized 4 bits, to merge the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe985335-acfa-4ea2-95de-05fd4088286f",
   "metadata": {
    "id": "fe985335-acfa-4ea2-95de-05fd4088286f"
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"Llama-3.2-1B-chat-doctor-QLoRA\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Uncomment the following to load the pretrained model if you did not manage to train your own\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     \"khengkok/Llama-3.2-1B-chat-doctor-QLoRA\",\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a981eebf-12cc-45b6-a05b-f54e2a59eb2e",
   "metadata": {
    "id": "a981eebf-12cc-45b6-a05b-f54e2a59eb2e"
   },
   "source": [
    "After merging the adapter with the base model, we can use it with the prompt template that we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7abdb-4b37-448a-a450-c2d5dd61476e",
   "metadata": {
    "id": "77f7abdb-4b37-448a-a450-c2d5dd61476e"
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"I have stomach pain. What should I do?\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=120, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b6d96-fbfd-4f13-b36f-6794999e4db0",
   "metadata": {
    "id": "8d4b6d96-fbfd-4f13-b36f-6794999e4db0"
   },
   "outputs": [],
   "source": [
    "# #Streaming support\n",
    "streamer = TextStreamer(tokenizer)\n",
    "merged_model.eval()\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    merged_model.generate(**model_input, streamer=streamer, max_new_tokens=250, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93314a3-1538-4f76-9331-179c5fa33bd7",
   "metadata": {
    "id": "d93314a3-1538-4f76-9331-179c5fa33bd7"
   },
   "source": [
    "### Saving and pushing the merged model\n",
    "\n",
    "We'll now save a tokenizer and model using the save_pretrained() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f356fb6c-6617-4803-b967-62dd0c0efaed",
   "metadata": {
    "id": "f356fb6c-6617-4803-b967-62dd0c0efaed"
   },
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(\"Llama-3.2-1B-chat-doctor\")\n",
    "tokenizer.save_pretrained(\"Llama-3.2-1B-chat-doctor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41367d1f-67d1-410e-a655-784ca288ecab",
   "metadata": {
    "id": "41367d1f-67d1-410e-a655-784ca288ecab"
   },
   "outputs": [],
   "source": [
    "merged_model.push_to_hub(\"Llama-3.2-1B-chat-doctor\", use_temp_dir=False)\n",
    "tokenizer.push_to_hub(\"Llama-3.2-1B-chat-doctor\", use_temp_dir=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
