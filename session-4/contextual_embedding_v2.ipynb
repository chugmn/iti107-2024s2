{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107-2024s2/blob/main/session-4/contextual_embedding_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKanFNbpTfv4"
      },
      "source": [
        "# Contextual Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8phFkS4Tfv7"
      },
      "source": [
        "One of the main drawbacks of embeddings such as Word2Vec and GloVE are that they have the same embedding for the same word regardless of its meaning in a particular context. For example, the word `rock` in `The rock concert is being held at national stadium` have a very different meaning in `The naughty boy throws a rock at the dog`.\n",
        "\n",
        "Contextual embedding such as those produced by transformers (where the modern-day large language are based on) took into account the context of the word, and different embedding is generated for the same word depending on the context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nte0QWOZwOAw"
      },
      "source": [
        "## Install Transformers library\n",
        "If you are running this notebook in Google Colab, you will need to install the Hugging Face transformers library as it is not part of the standard environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcAO5A0oVMOj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to generate some embeddings using one of the transformer model `deberta`."
      ],
      "metadata": {
        "id": "rO0i7U9ohSGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "# Load a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "82NPCKELP2cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a language model\n",
        "model = TFAutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer('The rock concert is being held at national stadium.', return_tensors='tf')\n",
        "print(tokens)\n",
        "for token in tokens['input_ids'][0]:\n",
        "    print(tokenizer.decode(token))"
      ],
      "metadata": {
        "id": "A0qLQXtCP4yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will pass the tokens through the model to generate embeddings.  We will take the embedding produced by the last layer."
      ],
      "metadata": {
        "id": "EdIhrOD8h0mY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the tokens\n",
        "embeddings_1 = model(**tokens)[0]\n",
        "print(embeddings_1)"
      ],
      "metadata": {
        "id": "e5BoSbY9QBRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions**\n",
        "\n",
        "1. What is the shape of the embeddings?\n",
        "2. Why is the shape is such?"
      ],
      "metadata": {
        "id": "pH_lvBExis_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to find the embedding of the token 'rock' used here."
      ],
      "metadata": {
        "id": "G8io3vzuiPHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_rock1 = embeddings_1[0][2]\n",
        "print(embedding_rock1)"
      ],
      "metadata": {
        "id": "kjqUWgowiVmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now write codes to find the embeddings of the word `rock` as used in the sentence `The naughty boy throws a rock at the dog.` and `A big rock falls from the slope after heavy rain.`.\n"
      ],
      "metadata": {
        "id": "3wgEKbkojDbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Click here for answer</summary>\n",
        "\n",
        "```\n",
        "tokens = tokenizer('The naughty boy throws a rock at the dog.', return_tensors='tf')\n",
        "print(tokens)\n",
        "for token in tokens['input_ids'][0]:\n",
        "    print(tokenizer.decode(token))\n",
        "embeddings_2 = model(**tokens)[0]\n",
        "embedding_rock2 = embeddings_2[0][6]\n",
        "```\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "TRdMzJBucY3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write code to extract embedding of rock for sentence \"The naughty boy throws a rock at the dog.\"\n",
        "# store the embedding as embedding_rock2\n",
        "\n",
        "embedding_rock2 = None"
      ],
      "metadata": {
        "id": "tYUkJJh4Qhzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Click here for answer</summary>\n",
        "\n",
        "```\n",
        "tokens = tokenizer('A big rock falls from the slope after heavy rain.', return_tensors='tf')\n",
        "print(tokens)\n",
        "for token in tokens['input_ids'][0]:\n",
        "    print(tokenizer.decode(token))\n",
        "embeddings_3 = model(**tokens)[0]\n",
        "embedding_rock3 = embeddings_3[0][3]\n",
        "```\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "_2COtAiHdT5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write code to extract embedding of rock for sentence \"A big rock falls from the slope after heavy rain.\"\n",
        "# store the embedding as embedding_rock3\n",
        "\n",
        "embedding_rock3 = None"
      ],
      "metadata": {
        "id": "2ESGR29ukgUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute how similar are the embeddings to each other"
      ],
      "metadata": {
        "id": "E9JKxtJWk0TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.losses import CosineSimilarity\n",
        "\n",
        "cos = CosineSimilarity(axis=0)\n",
        "similarity1 = cos(embedding_rock1, embedding_rock2)\n",
        "# invert the negative\n",
        "print(-similarity1)\n",
        "\n",
        "similarity2 = cos(embedding_rock2, embedding_rock3)\n",
        "print(-similarity2)\n",
        "\n"
      ],
      "metadata": {
        "id": "dWcM7DqjQ6z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that embedding_rock2 are more similar to embedding_rock3 than with embedding_rock1."
      ],
      "metadata": {
        "id": "elwn5Tr9SrDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Text Classification Model with DistilBert Embeddings\n",
        "\n",
        "In the previous lab, we have trained a text classification model using pretrained context-free embeddings GloVE.\n",
        "\n",
        "In this exercise, we will replace the embeddings with embeddings produced by DistilBERT model and compare the performance."
      ],
      "metadata": {
        "id": "1sKRicbAVZ8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the dataset\n",
        "\n",
        "Instead of using 10000 samples as before, we will just use 2000 samples for training."
      ],
      "metadata": {
        "id": "oezBA5sQWERh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh7XepZEZ3Ll"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# downloaded the datasets.\n",
        "test_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv'\n",
        "train_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_data_url)\n",
        "test_df = pd.read_csv(test_data_url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 2500\n",
        "TEST_SIZE = 500\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "train_df = train_df.sample(n=TRAIN_SIZE, random_state=128)\n",
        "test_df = test_df.sample(n=TEST_SIZE, random_state=128)\n",
        "\n",
        "# convert the text label to numeric label\n",
        "train_df['sentiment'] =  train_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
        "test_df['sentiment'] =  test_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)"
      ],
      "metadata": {
        "id": "k_uwtaepv1ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=128)"
      ],
      "metadata": {
        "id": "A0R8-DTZwnBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = train_df['review'].to_list()\n",
        "train_labels = train_df['sentiment'].to_list()\n",
        "val_texts = val_df['review'].to_list()\n",
        "val_labels = val_df['sentiment'].to_list()\n",
        "test_texts = test_df['review'].to_list()\n",
        "test_labels = test_df['sentiment'].to_list()"
      ],
      "metadata": {
        "id": "OSgOTn5giNK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_texts)"
      ],
      "metadata": {
        "id": "fQN7f_7wiXmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmboqVJWTfwA"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "We will now load the DistilBert tokenizer for the pretrained model \"distillbert-base-uncased\".  This is the same as the other lab exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5THnkPITfwA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = TFAutoModel.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, padding=True, truncation=True)\n",
        "test_encodings = tokenizer(test_texts, padding=True, truncation=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "Rj72APmabuBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        ")).batch(batch_size)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        ")).batch(batch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        ")).batch(batch_size)"
      ],
      "metadata": {
        "id": "s041QXJYi4h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(dataset):\n",
        "\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    for encoding, label in dataset:\n",
        "        output = model(encoding)\n",
        "        sentence_embedding = tf.reduce_mean(output[0], axis=1).numpy()\n",
        "        embeddings.append(sentence_embedding)\n",
        "        labels.append(label)\n",
        "\n",
        "    embeddings, labels = np.concatenate(embeddings), np.concatenate(labels)\n",
        "\n",
        "    return embeddings, labels"
      ],
      "metadata": {
        "id": "NyH-QjMwjDhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = extract_features(train_dataset)\n",
        "X_val, y_val = extract_features(val_dataset)\n",
        "X_test, y_test = extract_features(test_dataset)"
      ],
      "metadata": {
        "id": "4CU2zBxjjF4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVFbiO3STfwB"
      },
      "source": [
        "Here we will tokenize the text string, and pad the text string to the longest sequence in the batch, and also to truncate the sequence if it exceeds the maximum length allowed by the model (in BERT's case, it is 512)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbhAG0WnTfwD"
      },
      "source": [
        "## Train a classifier using the extracted features (embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiA0JMdzTfwE"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    # tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "THCqvmiNkgfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
        "\n",
        "def get_run_logdir():    # use a new directory for each run\n",
        "\timport time\n",
        "\trun_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "\treturn os.path.join(root_logdir, run_id)\n",
        "\n",
        "run_logdir = get_run_logdir()\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=run_logdir)\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"bert_best.weights.h5\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n"
      ],
      "metadata": {
        "id": "RA8jh52Tkc8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=32,\n",
        "    epochs=30,\n",
        "    callbacks=[tensorboard_callback, model_checkpoint_callback])"
      ],
      "metadata": {
        "id": "ILtEAUpCkjii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyaH5WnxTfwE"
      },
      "source": [
        "We should be getting an validation accuracy score of around 86% which is quite good, considering we are training with only 2000 samples!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate it on the test set."
      ],
      "metadata": {
        "id": "y81ly3-lUR2C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohs_qdbrthxv"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\"bert_best.weights.h5\")\n",
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Sentence Transformer\n",
        "\n"
      ],
      "metadata": {
        "id": "1pyphBfDjfWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "j7yj6O_EgMUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "EKZl9g-ogeud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "oZHEsx71jybz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load model\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Convert text to embeddings\n",
        "train_embeddings = model.encode(train_df[\"review\"], show_progress_bar=True)\n",
        "val_embeddings = model.encode(val_df['review'], show_progress_bar=True)\n",
        "test_embeddings = model.encode(test_df[\"review\"], show_progress_bar=True)"
      ],
      "metadata": {
        "id": "rCEYYGA3f5KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_embeddings.shape"
      ],
      "metadata": {
        "id": "nQw8GKcwiVvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    # tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "yOjSsAU6iUVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "E0CB89hygroa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
        "\n",
        "def get_run_logdir():    # use a new directory for each run\n",
        "\timport time\n",
        "\trun_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "\treturn os.path.join(root_logdir, run_id)\n",
        "\n",
        "run_logdir = get_run_logdir()\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=run_logdir)\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"sentence_transformer.weights.h5\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n"
      ],
      "metadata": {
        "id": "uaZw3EJ-igKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "2lr9KO9Ji2KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    train_embeddings, train_df['sentiment'],\n",
        "    validation_data=(val_embeddings, val_df['sentiment']),\n",
        "    batch_size=32,\n",
        "    epochs=30,\n",
        "    callbacks=[tensorboard_callback, model_checkpoint_callback])"
      ],
      "metadata": {
        "id": "5LeWe0IviiW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"sentence_transformer.weights.h5\")\n",
        "model.evaluate(test_embeddings, test_df['sentiment'])"
      ],
      "metadata": {
        "id": "l3d7Y8RyioHW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}